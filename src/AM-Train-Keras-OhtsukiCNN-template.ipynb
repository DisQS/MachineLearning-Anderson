{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anderson model of localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L50-5000-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L50-5000-s100 \n",
      " Keras-OhtsukiConv2D-111111\n",
      "/Users/danlo/Documents/PX319MLPhases_Data/L50-5000-s100 ./L50-5000-s100/Keras-OhtsukiConv2D-111111-model-L50-5000-s100.h5 ./L50-5000-s100/Keras-OhtsukiConv2D-111111-history-L50-5000-s100.pkl /Users/danlo/Documents/PX319MLPhases_Data/\n"
     ]
    }
   ],
   "source": [
    "myseed= 111111\n",
    "width= 50\n",
    "nimages= 100\n",
    "img_sizeX= 200\n",
    "img_sizeY= img_sizeX\n",
    "\n",
    "validation_split= 0.1\n",
    "batch_size= 64\n",
    "myepochs= 30\n",
    "mylr= 0.01\n",
    "mywd= 1e-6\n",
    "\n",
    "datanameformat=\"L{0}-{1}-s{2}\"\n",
    "dataname=datanameformat.format(width,nimages,img_sizeX)\n",
    "dataname=\"L50-5000-s100\"\n",
    "#datapath = '/storage/disqs/'+'ML-Data/Anderson/Images/'+dataname # SC-RTP\n",
    "#datapath = '/Users/danlo/ownCloud/' + dataname # Windows home\n",
    "#homepath = '/Users/danlo/Documents/PX319MLPhases_Data/'\n",
    "\n",
    "datapath = '/Users/danlo/Documents/PX319MLPhases_Data/' + dataname\n",
    "homepath = '/Users/danlo/Documents/PX319MLPhases_Data/'\n",
    "\n",
    "methodformat=\"Keras-OhtsukiConv2D-{0}\"\n",
    "method=methodformat.format(myseed)\n",
    "print(dataname,\"\\n\",datapath,\"\\n\",method)\n",
    "\n",
    "modelname = method+'-model-'+dataname+'.h5'\n",
    "historyname = method+'-history-'+dataname+'.pkl'\n",
    "\n",
    "savepath = './'+dataname+'/'\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(savepath)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "modelpath = savepath+modelname\n",
    "historypath = savepath+historyname\n",
    "\n",
    "print(datapath,modelpath,historypath,homepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard notebook settings\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard libraries\n",
    "import numpy as np\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "import random as rn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#np.random.seed(1337) # for reproducibility\n",
    "#np.random.seed(2000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:  2.1.0 , keras:  2.3.1\n",
      "sklearn:  0.23.2\n"
     ]
    }
   ],
   "source": [
    "#machine learning libraries\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "print(\"tensorflow: \",tf.__version__, \", keras: \", keras.__version__)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"sklearn: \", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#special subroutines\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D\n",
    "from keras.layers import Conv1D, MaxPooling2D\n",
    "from keras.layers import AveragePooling2D, Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "\n",
    "# from tensorflow.keras.layers import Dense, Conv2D\n",
    "# from tensorflow.keras.layers import Conv1D, MaxPooling2D\n",
    "# from tensorflow.keras.layers import AveragePooling2D, Flatten\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "# from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## starting the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(myseed) # necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "rn.seed(myseed+1) # necessary for starting core Python generated random numbers in a well-defined state.\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/fchollet/keras/issues/2280#issuecomment-306959926\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.compat.v2.random.set_seed(myseed+3)\n",
    "#tf.set_random_seed(1234)\n",
    "\n",
    "#sess = tf.compat.v2.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "#K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## reading the images"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,validation_split=validation_split)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "training_set = train_datagen.flow_from_directory(datapath,\n",
    "                                                 subset='training',\n",
    "                                                 target_size = (img_sizeX,img_sizeY),\n",
    "                                                 batch_size = batch_size, \n",
    "                                                 class_mode='categorical',\n",
    "                                                shuffle=True,seed=myseed)\n",
    "\n",
    "validation_set= train_datagen.flow_from_directory(datapath, \n",
    "                                              subset='validation', \n",
    "                                              target_size = (img_sizeX,img_sizeY),\n",
    "                                              batch_size = batch_size,\n",
    "                                              class_mode='categorical',\n",
    "                                                 shuffle=False,seed=myseed)\n",
    "\n",
    "# test_set = test_datagen.flow_from_directory('data-keras-L20-100/test_set',\n",
    "#                                             target_size = (171, 171),\n",
    "#                                             batch_size = batch_size,\n",
    "#                                             class_mode='categorical',\n",
    "#                                            shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_labels = next(training_set)\n",
    "# Y_train, Y_labels = next(validation_set)\n",
    "# len(X_train),len(X_labels),len(Y_train),len(Y_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "num_of_train_samples = training_set.samples\n",
    "num_of_test_samples = validation_set.samples\n",
    "num_classes = len(validation_set.class_indices)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    for x,y in validation_set:\n",
    "        plt.imshow(x[0],cmap='hsv')\n",
    "        #plt.title('y={}'.format(y[0]))\n",
    "        plt.axis('off')\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN architecture (Ohtsuki) created successfully!\n"
     ]
    }
   ],
   "source": [
    "def create_CNN():\n",
    "    # instantiate model\n",
    "    model=Sequential()\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=(5,5),input_shape=(img_sizeX, img_sizeY, 3),\n",
    "                     activation = 'relu',  padding='valid'))\n",
    "    model.add(Conv2D(16, kernel_size=(5,5),activation = 'relu',padding='same'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size =(2, 2), padding='same'))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "               \n",
    "    model.add(Conv2D(32,kernel_size=(3,3), activation = 'relu', padding='valid'))  \n",
    "    model.add(Conv2D(32,kernel_size=(3,3), activation = 'relu', padding='same')) \n",
    "                 \n",
    "    model.add(MaxPooling2D(pool_size =(2, 2),padding='same')) \n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(5,5), activation = 'relu',  padding='valid'))\n",
    "    model.add(Conv2D(64, kernel_size=(5,5),activation = 'relu',padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size =(2, 2),padding='same'))\n",
    "                  \n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "                             \n",
    "    model.add(Dense(units = 64, activation = 'relu'))\n",
    "    model.add(Dense(units = num_classes, activation = 'softmax')) \n",
    "    \n",
    "    return model\n",
    "\n",
    "print('CNN architecture (Ohtsuki) created successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled successfully and ready to be trained.\n"
     ]
    }
   ],
   "source": [
    "# Choose the Optimizer and the Cost Function\n",
    "\n",
    "opt = optimizers.SGD(lr=mylr, decay=mywd)\n",
    "#opt = keras.optimizers.Adam(lr=mylr, decay=mywd)\n",
    "\n",
    "def compile_model(optimizer=opt):\n",
    "    # create the mode\n",
    "    model=create_CNN()\n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print('Model compiled successfully and ready to be trained.')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# opt = optimizers.SGD(lr=mylr, decay=mywd)\n",
    "# model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# create the deep neural net\n",
    "model = compile_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L20-5000-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L20-5000-s100 \n",
      " Keras-OhtsukiConv2D-111111\n",
      "Found 76500 images belonging to 18 classes.\n",
      "Found 8500 images belonging to 18 classes.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 196, 196, 16)      1216      \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 196, 196, 16)      6416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 98, 98, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 98, 98, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 96, 96, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 96, 96, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 44, 44, 64)        51264     \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 44, 44, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                1982528   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 18)                1170      \n",
      "=================================================================\n",
      "Total params: 2,158,946\n",
      "Trainable params: 2,158,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "1195/1195 [==============================] - 891s 746ms/step - loss: 2.8252 - accuracy: 0.0690 - val_loss: 2.8010 - val_accuracy: 0.0846\n",
      "133/133 [==============================] - 26s 198ms/step\n",
      "Accuracy:  0.08411764353513718\n",
      "L30-5000-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L30-5000-s100 \n",
      " Keras-OhtsukiConv2D-111111\n",
      "Found 76500 images belonging to 18 classes.\n",
      "Found 8500 images belonging to 18 classes.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 196, 196, 16)      1216      \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 196, 196, 16)      6416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 98, 98, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 98, 98, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 96, 96, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 96, 96, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 44, 44, 64)        51264     \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 44, 44, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                1982528   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 18)                1170      \n",
      "=================================================================\n",
      "Total params: 2,158,946\n",
      "Trainable params: 2,158,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "1195/1195 [==============================] - 902s 755ms/step - loss: 2.6671 - accuracy: 0.1119 - val_loss: 2.4210 - val_accuracy: 0.1299\n",
      "133/133 [==============================] - 25s 187ms/step\n",
      "Accuracy:  0.12917646765708923\n",
      "L40-5000-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L40-5000-s100 \n",
      " Keras-OhtsukiConv2D-111111\n",
      "Found 76500 images belonging to 18 classes.\n",
      "Found 8500 images belonging to 18 classes.\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 196, 196, 16)      1216      \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 196, 196, 16)      6416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 98, 98, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 98, 98, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 96, 96, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 96, 96, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 44, 44, 64)        51264     \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 44, 44, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                1982528   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 18)                1170      \n",
      "=================================================================\n",
      "Total params: 2,158,946\n",
      "Trainable params: 2,158,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "1195/1195 [==============================] - 896s 750ms/step - loss: 2.5841 - accuracy: 0.1239 - val_loss: 2.3046 - val_accuracy: 0.1482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 25s 186ms/step\n",
      "Accuracy:  0.1504705846309662\n",
      "L50-5000-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L50-5000-s100 \n",
      " Keras-OhtsukiConv2D-111111\n",
      "Found 64584 images belonging to 15 classes.\n",
      "Found 7176 images belonging to 15 classes.\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_31 (Conv2D)           (None, 196, 196, 16)      1216      \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 196, 196, 16)      6416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 98, 98, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 98, 98, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 96, 96, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 96, 96, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 44, 44, 64)        51264     \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 44, 44, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                1982528   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 15)                975       \n",
      "=================================================================\n",
      "Total params: 2,158,751\n",
      "Trainable params: 2,158,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "1009/1009 [==============================] - 748s 741ms/step - loss: 2.4205 - accuracy: 0.1597 - val_loss: 1.8794 - val_accuracy: 0.1888\n",
      "113/113 [==============================] - 21s 186ms/step\n",
      "Accuracy:  0.1892419159412384\n",
      "Accuracy list:  [0.08411764353513718, 0.12917646765708923, 0.1504705846309662, 0.1892419159412384]\n"
     ]
    }
   ],
   "source": [
    "# train DNN and store training info in history\n",
    "#sys_size_list = [20,30,40,50];\n",
    "#for (sys_size in sys_size_list):\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "sys_scores = []\n",
    "sys_loss   = []\n",
    "sys_sizes = ['20','30','40','50']\n",
    "\n",
    "for sys_size in sys_sizes:\n",
    "    dataname=\"L\"+sys_size+\"-5000-s100\"\n",
    "    datapath = '/Users/danlo/Documents/PX319MLPhases_Data/' + dataname\n",
    "    homepath = '/Users/danlo/Documents/PX319MLPhases_Data/'\n",
    "\n",
    "    methodformat=\"Keras-OhtsukiConv2D-{0}\"\n",
    "    method=methodformat.format(myseed)\n",
    "    print(dataname,\"\\n\",datapath,\"\\n\",method)\n",
    "\n",
    "    modelname = method+'-model-'+dataname+'.h5'\n",
    "    historyname = method+'-history-'+dataname+'.pkl'\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1./255,validation_split=validation_split)\n",
    "    test_datagen  = ImageDataGenerator(rescale=1./255)\n",
    "     \n",
    "    training_set = train_datagen.flow_from_directory(datapath,\n",
    "                                                 subset='training',\n",
    "                                                 target_size = (img_sizeX,img_sizeY),\n",
    "                                                 batch_size = batch_size, \n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True,seed=myseed)\n",
    "\n",
    "    validation_set= train_datagen.flow_from_directory(datapath, \n",
    "                                                  subset='validation', \n",
    "                                                  target_size = (img_sizeX,img_sizeY),\n",
    "                                                  batch_size = batch_size,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False,seed=myseed)\n",
    "\n",
    "    num_of_train_samples = training_set.samples\n",
    "    num_of_test_samples = validation_set.samples\n",
    "    num_classes = len(validation_set.class_indices)\n",
    "    \n",
    "    model = compile_model()\n",
    "    #print(model.summary())\n",
    "    \n",
    "        #Train the CNN\n",
    "    history = model.fit_generator(training_set,\n",
    "                         steps_per_epoch = training_set.samples // batch_size,\n",
    "                         epochs = myepochs,\n",
    "                         validation_data = validation_set,\n",
    "                         validation_steps = validation_set.samples // batch_size)\n",
    "    \n",
    "    score = model.evaluate(validation_set, verbose=1)\n",
    "    sys_scores.append(score[1])\n",
    "    sys_loss.append(score[0])\n",
    "    print(\"Accuracy: \", score[1])\n",
    "    print(\"loss: \", score[0])\n",
    "    model.save(modelpath) \n",
    "        \n",
    "print(\"Accuracy list: \", sys_scores)\n",
    "fig=plt.figure()\n",
    "plt.plot(sys_size, sys_scores)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('system_size')\n",
    "plt.show()\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.plot(sys_size, sys_loss)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('system_size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.models.save_model(history,'Anderson_Ohtsuki_model_L20_500_keras_SGD_0_01_good_input_size.h5') \n",
    "model.save(modelpath) \n",
    "\n",
    "import pickle\n",
    "f=open(historypath,\"wb\")\n",
    "pickle.dump(history,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing the quality of the learned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping the Training, saving inbetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:tensorflow:From <ipython-input-38-7d86d904df6f>:8: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1009 steps, validate for 112 steps\n",
      "Epoch 1/10\n",
      "1009/1009 [==============================] - 723s 716ms/step - loss: 2.2165 - accuracy: 0.2011 - val_loss: 2.2354 - val_accuracy: 0.2041\n",
      "Epoch 2/10\n",
      "1009/1009 [==============================] - 724s 718ms/step - loss: 2.1836 - accuracy: 0.2104 - val_loss: 2.3000 - val_accuracy: 0.1881\n",
      "Epoch 3/10\n",
      "1009/1009 [==============================] - 725s 718ms/step - loss: 2.1633 - accuracy: 0.2143 - val_loss: 2.2719 - val_accuracy: 0.1968\n",
      "Epoch 4/10\n",
      "1009/1009 [==============================] - 726s 720ms/step - loss: 2.1522 - accuracy: 0.2165 - val_loss: 2.3297 - val_accuracy: 0.1871\n",
      "Epoch 5/10\n",
      "1009/1009 [==============================] - 727s 720ms/step - loss: 2.1393 - accuracy: 0.2179 - val_loss: 2.3218 - val_accuracy: 0.1867\n",
      "Epoch 6/10\n",
      "1009/1009 [==============================] - 727s 721ms/step - loss: 2.1284 - accuracy: 0.2240 - val_loss: 2.2833 - val_accuracy: 0.2017\n",
      "Epoch 7/10\n",
      "1009/1009 [==============================] - 747s 741ms/step - loss: 2.1208 - accuracy: 0.2264 - val_loss: 2.2594 - val_accuracy: 0.2048\n",
      "Epoch 8/10\n",
      "1009/1009 [==============================] - 749s 742ms/step - loss: 2.1125 - accuracy: 0.2287 - val_loss: 2.2748 - val_accuracy: 0.1929\n",
      "Epoch 9/10\n",
      "  48/1009 [>.............................] - ETA: 11:20 - loss: 2.0965 - accuracy: 0.2301"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "for i in range(2):\n",
    "    model = load_model(modelpath)\n",
    "    history = model.fit_generator(training_set,\n",
    "                         steps_per_epoch = training_set.samples // batch_size,\n",
    "                         epochs = 10,\n",
    "                         validation_data = validation_set,\n",
    "                         validation_steps = validation_set.samples // batch_size)\n",
    "model.save(modelpath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score=model.evaluate(validation_set,verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# look into training history\n",
    "\n",
    "# summarize history for accuracy\n",
    "fig=plt.figure()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.ylabel('model accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.title(dataname)\n",
    "plt.show()\n",
    "fig.savefig(datapath+'/'+method+dataname+'_accuracy'+'.png')\n",
    "\n",
    "# summarize history for loss\n",
    "fig=plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.title(dataname)\n",
    "plt.show()\n",
    "fig.savefig(datapath+'/'+method+dataname+'_loss'+'.png')\n",
    "#print(datapath+'/'+surname+dataname+'_loss'+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set.reset()\n",
    "label=validation_set.class_indices.keys()\n",
    "\n",
    "#Confusion Matrix \n",
    "Y_pred = model.predict_generator(validation_set, num_of_test_samples // batch_size+1, verbose=1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "#basic confusion matrix\n",
    "confusion_matrix(validation_set.classes, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir('../PyCode/')\n",
    "from plot_confusion_matrix import *\n",
    "\n",
    "print(plot_confusion_matrix(confusion_matrix(validation_set.classes, y_pred),\n",
    "                          label,\n",
    "                          title='Confusion matrix for '+dataname,\n",
    "                          cmap=None,\n",
    "                          normalize=True))\n",
    "os.chdir('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
